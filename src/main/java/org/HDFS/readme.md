# Hadoop HDFS

## 数据持久化

1) 日志文件: 记录实时发生的增删改事件，已追加(append)的形式写入文本文件中，完整性比较好，但恢复数据消耗时间很长。
2) 镜像、快照: 内存全量数据基于某一时间节点进行磁盘溢写操作，因为时间节点间有时间间隔，所以会丢失数据，但是恢复数据消耗时间较短。
3) HDFS: 使用 EditsLog(日志) + FsImage(快照) 结合的形式

## HDFS 持久化操作

### 宕机恢复

1) 如果设置 每隔一小时进行一次快照，当服务器在 9：40 重启后。
2) 会读取 9点 的 FsImage + 9点后 EditsLog 中的事件 即可恢复到宕机前的数据。

### 正常持久化

1) NameNode 在 8：20 第一次开启后，只写一次 FsImage， 执行时间到9点， EditsLog会将 8：20至9：00的日志，更新到 8点 的FsImage上，
   FsImage上的时间节点就从8点变为9点。

## 架构

### Client

1) 切片操作

### NameNode

1) 用于存储元数据： 文件位置和文件属性
2) 持久化时只会持久化文件属性，文件位置不会持久化，当 NameNode 再次启动后，根据DataNode 上报的心跳重新获取文件位置。
   目的是为了确保分布式情况下数据的一致性。
3) 放置在内存，所以在配置时需要大量的内存。

### DataNode

1) 存储文件的块数据
2) 块数据的校验和

### SecondDataNode

1) 辅助NameNode对元数据的管理
2) 管理流程:
    1) 第一阶段：启动NameNode
        1) 如果是首次启动namenode格式化，则新建fsimage（镜像文件）和edits log（编辑日志）
           如果是非首次启动，则直接加载fsimage和edits log到内存中。
        2) 客户端对元数据的增删改操作会实时的写入到edits log
    2) 第二阶段：SecondaryNameNode开始工作
        1) SecondaryNameNode会实时检查edits log的状态，只要满足一定阈值时（1小时或修改达到100W次）后就通知NameNode重新生成一个新的edits
           log文件，后续将操作记录写入新文件中
        2) SecondaryNameNode通过HTTP协议拉取NameNode中的fsimage和edits log到本地
        3) 对拉取过来的edits log和fsimage加载到内存中进行合并操作（这个过程也成为Checkpoint），形成新的fsimage文件
        4) 把新的fsimage推送给NameNode，替换旧fsimage

### 在 HA 情况下

1) 会利用 Zookeeper 进行 多个NameNode 切换
2) 备用 NameNode 会进行 元数据的持久化工作。

## 数据读写

### 读取文件

1) 客户端向NameNode请求读取文件

2) NameNode检查该文件是否存在以及该客户端是否具有读权限，有一个不满足则返回报错信息
   两者都有则根据“机架感知原理”和“网络拓补图”，返回存储该文件的块地址（存储该文件的DataNode列表）

3) 客户端拿到返回的块地址后，并行的读取DataNode列表中对应的块信息
   如果之前读取的是部分块的信息，则在这些块数据读取完毕后会重新请求NameNode 获取 剩下的块地址重新读取，直至所有数据块的信息读取完毕

4) 最后拼接块信息得到最终的文件

### 写入文件

1) 客户端向NameNode请求上传文件

2) NameNode检查是否已存在要上传的文件，如果已有则拒绝请求
   如文件不存在则继续检查该客户端在待上传的目录下是否有写权限，如果无权限则返回报错信息，
   有权限则给客户端返回可以上传的信息

3) 客户端接收可以上传的信息后，对文件进行切块

4) 客户端重新请求NameNode，询问第一个数据块的上传位置

5) NameNode接收到客户端的请求后，根据副本机制、负载均衡、机架感知原理和网络拓补图，找到存储第一个数据块的DataNode列表（例如node1、node2、node3）后告知客户端

6) 客户端根据接收到的DataNode列表，连接就近的节点（例如node1）

7) 第一个节点收到请求后会与DataNode列表中的其他节点进行连接，形成“传输管道”，然后客户端通过数据报包（对数据块再进行切分）的方法开始给节点传输第一个数据块

8) 节点接收到数据块后，需要告知客户端块信息已上传成功
   所以node3接收到信息后会反馈给node2已接收，node2再反馈给node1已接收，最后node1告知客户端已上传成功, 这一步也称为【构建反向应答机制】

9) 第一个数据块上传完成后，客户端继续请求NameNode询问第二个数据块的上传位置，重复第四到第八步的操作，直至所有的数据块上传成功

## 五大机制

### 切片机制

1) HDFS中的文件在物理上是分块（block）存储的，块的大小可以通过配置参数来规定，在hadoop2.x版本中默认大小是128M

### 汇报机制

1) HDFS集群重新启动的时候，所有的DataNode都要向NameNode汇报自己的块信息
2) 当集群在正常工作的时，间隔一定时间（6小时）后DataNode也要向NameNode汇报一次自己的块信息

### 心跳监测机制

1) DataNode每3秒给NameNode发送自己的心跳信息
2) 如果NameNode没有收到心跳信息，则认为DataNode进入“假死”状态。DataNode在此阶段还会再尝试发送10次（30s）心跳信息
3) 如果NameNode超过最大间隙时间（10分钟）还未接收到DataNode的信息，则认为该DataNode进入“宕机”状态
4) 当检测到某个DataNode宕机后，NameNode会将该DataNode存储的所有数据重新找台活跃的新机器做备份

### 负载均衡

1) 让集群中所有的节点（服务器）的利用率和副本数尽量都保持一致或在同一个水平线上

### 副本机制

1) 副本的默认数量为3
2) 当某个块的副本小于3份时，NameNode会新增副本
3) 当某个块的副本大于3份时，NameNode会删除副本
4) 当某个块的副本数小于3份且无法新增的时候，此时集群会强制进入安全模式（只能读，不能写）

## 知识点

### 机架感知 + 网络拓扑结构 实现副本摆放

1) 第1个副本：优先本机存放，否则就近随机
2) 第2个副本：放在与第1个副本就近不同机架上的某一个服务器
3) 第3个副本：与第2个副本相同机架的不同服务器。
4) 如果还有更多的副本：随机放在各机架的服务器中。

### 副本分发

1) 客户端接收到 DataNode 机器列表后会依据网络拓扑的原理找到其中一台机器进行传输通道的建立，然后依次和三台机器进行串行连接，这样的连接方式主要的为了减轻客户端本地的IO的压力。